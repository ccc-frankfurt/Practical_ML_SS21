{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression_Titanic_dataset",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccc-frankfurt/Practical_ML_SS21/blob/main/week02/Regression_Titanic_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3fYh7yvuhzO"
      },
      "source": [
        "# Regression and the Titanic dataset\n",
        "\n",
        "Welcome to this week's session where we will explore the Titanic dataset and apply what we have learned about linear and logistic regression and gradient descent methods. \n",
        "\n",
        "You can find the Titanic dataset and a corresponding tutorial challenge on Kaggle at https://www.kaggle.com/c/titanic "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfenGnZLFMUG"
      },
      "source": [
        "Let's first import the libraries we will need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftb-COR2ttbI"
      },
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlyClR1TFT6Y"
      },
      "source": [
        "and load the dataset. Luckily the dataset is available through seaborns load_dataset function, but you can also find it on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPY-84n6vZIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "4bd9b914-994e-4ca9-fd00-410369c7c700"
      },
      "source": [
        "titanic = sns.load_dataset(\"titanic\")\n",
        "print(titanic.info())\n",
        "print(titanic.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 15 columns):\n",
            "survived       891 non-null int64\n",
            "pclass         891 non-null int64\n",
            "sex            891 non-null object\n",
            "age            714 non-null float64\n",
            "sibsp          891 non-null int64\n",
            "parch          891 non-null int64\n",
            "fare           891 non-null float64\n",
            "embarked       889 non-null object\n",
            "class          891 non-null category\n",
            "who            891 non-null object\n",
            "adult_male     891 non-null bool\n",
            "deck           203 non-null category\n",
            "embark_town    889 non-null object\n",
            "alive          891 non-null object\n",
            "alone          891 non-null bool\n",
            "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
            "memory usage: 80.6+ KB\n",
            "None\n",
            "         survived      pclass         age       sibsp       parch        fare\n",
            "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
            "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
            "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
            "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
            "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
            "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
            "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
            "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldMNQRXjF35c"
      },
      "source": [
        "## Inspecting the Titanic dataset and preparing it for machine learning\n",
        "\n",
        "Before we continue with machine learning algorithms, let us take a closer look at the dataset and select suitable features for our regression. \n",
        "\n",
        "A good first step is to count the number of entries per feature.\n",
        "\n",
        "**Print the count**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPnYXtlbwbF-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d3f914d-5986-4e49-d386-5b17a4f95f6e"
      },
      "source": [
        "print(\"Total count per category\")\n",
        "# print the total count per category"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total count per category\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6fj51_HGScr"
      },
      "source": [
        "We can observe that the passenger data has some **incomplete entries** and **some features are missing for certain passengers**. \n",
        "\n",
        "In addition some entries are lacking more than others. \n",
        "\n",
        "For the overall 891 passangers there is only 2 values missing for the catefories *embarked* and *embark_town*. In other words, we could easily adapt the dataset to include these two features by excluding these two passangers from our list of features. \n",
        "\n",
        "Other categories such as *age* and *deck* are much more difficult to leverage. We do not have any means to fill in the missing data and it is a bad idea to throw away the majority of our passenger data if we wanted to use the *deck* feature. The best idea is thus to not consider the *deck* feature in our further analysis.\n",
        "\n",
        "Let's drop the two passengers with missing *embarked* and *embark_town* entries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SiXYrQcIkuI"
      },
      "source": [
        "titanic = # drop the entries with NaN values for the embarked and embarked_town columns\n",
        "\n",
        "print(titanic.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb8baVIvK2y7"
      },
      "source": [
        "The next thing we observe is that some features we want to use do not have numerical values but are stored as strings. \n",
        "\n",
        "This is the case for the embarked feature that we have just cleaned where passengers are stored into three categories *C, Q, S* depending on whether they have embarked in one of three towns: C = Cherbourg, Q = Queenstown, S = Southampton.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYz3V-kwKzFd"
      },
      "source": [
        "print(titanic[\"embarked\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9_qVxunLdwf"
      },
      "source": [
        "What we often do in machine learning is that we can create a numerical **embedding** where we map the strings to numbers. In our case we can choose a quite simple one and convert **C, Q, S into 0, 1, 2**.\n",
        "\n",
        "\n",
        "Luckily pandas offers a function for this named *replace({'string': num})*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wST5AidoGN7V"
      },
      "source": [
        "# check if already converted because the cell will otherwise\n",
        "# crash if executed multiple times\n",
        "if isinstance(titanic[\"embarked\"][0], str):\n",
        "    titanic[\"embarked\"] = # replace C, Q, S strings with 0, 1, 2 integer values\n",
        "print(titanic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhQHVVcZtVsI"
      },
      "source": [
        "We can also notice that some features exist multiple times in the data frame in multiple repsentations such as *pclass* and *class*, once being encoded numerically and with a string. We will simply select only one of these categories later for our final dataset. \n",
        "\n",
        "Let's visuallize  some of the features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpn2AHlxtNBZ"
      },
      "source": [
        "print(\"\\n Female only count\")\n",
        "# print the female only count per category. You can use the string match function \"str.match()\"\n",
        "print(titanic[titanic['sex'].str.match('female')].count())\n",
        "\n",
        "print(\"\\n Male only count\")\n",
        "# print the male only count per category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmd3MNJugLp"
      },
      "source": [
        "We can see that there was almost twice as many males as females on the titanic. \n",
        "\n",
        "If we take a look at the age distribution in a box plot with depicted mean and variances, we can see that age is almost evenly distributed, with some outliers in males aged older than 70. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKHqF3rUxYt1"
      },
      "source": [
        "plt.figure()\n",
        "# create a seaborn box plot with sex vs age"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WYfMSNtvELn"
      },
      "source": [
        "Let's proceed with taking a visual look at survival rates. Intuitively a good feature for survival rate would be the gender and class of a passenger.\n",
        "\n",
        "We can display both in a histogram. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVXaakWLur0n"
      },
      "source": [
        "# create a histgram with seaborn for the survived category for each class \n",
        "\n",
        "# create a histgram with seaborn for the survived category and the \"who\" category"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLhiWnirwE4R"
      },
      "source": [
        "We could already build our first hand-engineered simple classifier. From the figures we can observe that survival is very much related to being a female first class passenger.  To provide a better overview we will do this later and compare it with our logistic regression results. \n",
        "\n",
        "We are now ready to build our dataset with our selected features and write the code for the logistic regression itself. As a first subset we will select the features *sex, fare, parch, pclass and sibsp (number of siblings or spouses)*. For this we will do a final conversion of the *sex* column and drop one entry as each passenger has an entry for both female and male that are mutually exclusive (0, 1 or 1, 0). We can thus just drop one of the entries entirely.\n",
        "\n",
        "It's easiest if we concatenate (with *pd.concat*) the selected features into a new pandas data frame and then convert them to numpy matrices. The latter can be achieved by using the *pd.DataFrame.to_numpy(data_frame)* function. \n",
        "\n",
        "In order to have a format that is suitable for logistic regression we will also split the labels, in the titanic scenario the *survived* column, into a separate numpy array. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnPA5V4D4u4P"
      },
      "source": [
        "sex = pd.get_dummies(titanic[\"sex\"], drop_first=True)\n",
        "\n",
        "# subselect the categories sex, fare, parch, pclass, sibsp and concatenate them\n",
        "# into a new pandas dataframe called features\n",
        "\n",
        "features = ...\n",
        "print(features)\n",
        "\n",
        "x = # convert features to numpy array/matrix\n",
        "y = # convert the survived category into numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aPPU6-lyY_i"
      },
      "source": [
        "## Creation of training and test data splits\n",
        "The numpy matrices we have just created still lack multiple steps before we can treat them as datasets for logistic regression.\n",
        "    1. The passanger entries are sorted. This can be problematic if we use stochastic gradient descent methods as some features will be fed first. \n",
        "    2. We do not have a train and test split. Creation of this split is crucial as it gives us means to estimate whether our trained model can extrapolate to \"unseen\" data from the same data distribution, how much it generalizes and how much the model overfits, i.e. \"learns by heart\". \n",
        "    \n",
        "Let's create a held-out test set by first shuffling the dataset and then extracting 20% of the data. \n",
        "\n",
        "We will also have to reshape the labels from vectors to matrices because of shape expectations of numpy's internal matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1pNmhO46ybH"
      },
      "source": [
        "# create random permutation of indices of length of the dataset. \n",
        "# You can use np.random.permutation for this\n",
        "perm = ...\n",
        "\n",
        "\n",
        "# subselect 80% of the generated permuted indices to split the training data\n",
        "x_train = x[...] # first 80% of perm\n",
        "y_train = y[...]\n",
        "\n",
        "# use the remaining 20% to get a test split\n",
        "x_test = x[...] # remaining 20% of perm\n",
        "y_test = y[...]\n",
        "\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "# reshape the labels to fit later matrix multiplications (add a dummy dimension)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrr2Tptqzcuf"
      },
      "source": [
        "## Logistic regression\n",
        "\n",
        "We are ready for the final step. We will now need to implement the missing functionality for the logistic regression itself:\n",
        "    1. The sigmoid function  \n",
        "    2. The cross-entropy loss function\n",
        "    3. The gradient update rule\n",
        "    4. The training loop\n",
        "    5. A function to test accuracy on the dataset splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSznNdPwATfZ"
      },
      "source": [
        "Sigmoid function: $\\frac{1}{1 + e^{-z}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQGsBWmz0h5i"
      },
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the Sigmoid function\n",
        "    \n",
        "    Parameters:\n",
        "        z (float, np.array): scalar value or numpy array\n",
        "        \n",
        "    Returns:\n",
        "        float, np.array: sigmoid(x)\n",
        "    \"\"\"\n",
        "    \n",
        "    sig = # implement the sigmoid function\n",
        "    return sig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrTnsmIcAkvv"
      },
      "source": [
        "Cross-entropy loss: $−(y \\log(p)+(1−y)\\log(1−p))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbIRAcZS9vI9"
      },
      "source": [
        "def log_loss(y, y_hat):\n",
        "    \"\"\"\n",
        "    Cross-entropy loss function\n",
        "    \n",
        "    Parameters:\n",
        "        y (float, np.array): Labels or targets\n",
        "        y_hat (float, np.array): Predictions\n",
        "        \n",
        "    Returns:\n",
        "    float: log loss\n",
        "    \"\"\"\n",
        "    loss = -np.mean(...)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCDF-B-bA89O"
      },
      "source": [
        "Gradient update, for now let's implement the regular easiest gradient descent update:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHaWAfR7oHeZ"
      },
      "source": [
        "def grad_update(grad, lr):\n",
        "    \"\"\"\n",
        "    Gradient descent update rule\n",
        "    \n",
        "    Parameters:\n",
        "        grad (float, np.array): Array of gradients for each weight\n",
        "        lr (float): learning rate\n",
        "    \"\"\"\n",
        "    return lr * grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE7Wpe-AzzSB"
      },
      "source": [
        "We initialize the weights and bias to zero and set a number of epochs to train and a learning rate to use in the update. Good starting values can be 10000 epochs with a learning rate of 0.005. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y9R2Ju4-uCK"
      },
      "source": [
        "W = # initialize the weights with zeros. The shape should be feature_size, 1\n",
        "b = # initialize the biases with zeros. It is enough to use one bias value.\n",
        "\n",
        "num_epochs = 10000\n",
        "lr = 0.005\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    z = # do the linear weighting\n",
        "    A = # apply sigmoid function to get the prediction score\n",
        "    loss = # use your previously defined loss function to calculate the loss\n",
        "    losses.append(loss)\n",
        "    \n",
        "    # compute the gradients of the cost function with respect to the weights\n",
        "    dz = ...\n",
        "    dW = ...\n",
        "    db = ...\n",
        "    \n",
        "    W = # update the weights\n",
        "    b = # update the biases\n",
        "    \n",
        "    if epoch % 1000 == 0 and epoch > 0:\n",
        "        print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LgAEtB40ELq"
      },
      "source": [
        "We should have seen the loss decreasing over time. For the trained model we can now print the weights and bias values. Unfortunately because logistic regression weights aren't linearly, but multiplicatively combined, it is not straight forward to interpret these weights. \n",
        "\n",
        "Don't worry though, in the next weeks we will learn about interpretability and how to inspect models!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wer70jfBaOs"
      },
      "source": [
        "print(W, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J68LhqB44_I0"
      },
      "source": [
        "Let's visualize our losses. Because we have so many values, it is recommended to take every tenth value only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSbp4YHt4j3I"
      },
      "source": [
        "# plot every tenth value of the loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwsTdvq60cex"
      },
      "source": [
        "While looking at the losses can be meaningful, it doesn't really help our human interpretation of how well we can do on our task of predicting passenger survival. As our model is prediction survival score (like a probability in [0, 1] range.) we can binarize the prediction to yield *survived* if the output value is greater than 0.5. We can calculate this for the entire train and test dataset splits and see how well our model does in terms of accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXFh9E2VuDfG"
      },
      "source": [
        "def eval_acc(x, y):\n",
        "    acc = 0.0\n",
        "    \n",
        "    # loop through the entire dataset and accumulate accuracy\n",
        "    for inp in range(...):\n",
        "        # this will be the same as your training code, but without gradients\n",
        "        # or updates\n",
        "        z = ...\n",
        "        A = ...\n",
        "        \n",
        "        # check if the prediction matches the label. Note that we consider a \n",
        "        # label of 1 to match the prediction if the prediction is > 0.5 and vice\n",
        "        # versa\n",
        "        if ...:\n",
        "            acc += 1.0\n",
        "            \n",
        "    \n",
        "    # normalize the accumulated accuracy by the length of the dataset.\n",
        "    acc /= ...\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ6fVk6jBr2M"
      },
      "source": [
        "train_acc = # evaluate the train accuracy on the trained model\n",
        "test_acc = # evaluate the test accuracy on the trained model\n",
        "\n",
        "print(\"Training accuracy: \", train_acc)    \n",
        "print(\"Testing accuracy: \", test_acc)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrpCwOWD1Eey"
      },
      "source": [
        "Depending on the exact random split and our gradient descent parameters we can observe training and testing accuracies greater than 80%. If the training accuracy is higher and the test accuracy is low this means that the model is overfitting. If neither of the two accuracies go beyond random chance of 50% (a coin-flip of whether a passenger survived or not) then the model has not trained or the gradient descent got stuck in a local minimum. \n",
        "\n",
        "To an extend we can avoid gradient descent (GD) getting stuck in bad local minima or saddle points by making it stochastic (SGD). In the lecture we have encountered a method called mini-batch stochastic gradient descent, that balances the advantages of SGD stochasticity and convergence of GD. \n",
        "\n",
        "Let's pick a starting batch size of 200 and see how the training behavior changes. Because we are now doing many more updates than before, it is generally a good idea to lower the learning rate a little or train for an overall lesser amount of epochs. We do not change the number of epochs here but instead halve the learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EklL12ioFlml"
      },
      "source": [
        "W = # initialize the weights with zeros. The shape should be feature_size, 1\n",
        "b = # initialize the biases with zeros. It is enough to use one bias value.\n",
        "             \n",
        "num_epochs = 10000\n",
        "lr = 0.0025\n",
        "mb_size = 200\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # in stochastic gradient descent we shuffle the train dataset at the\n",
        "    # beginning of every epoch to avoid \n",
        "    perm = # create permuted integer indices for each dataset element \n",
        "    x_train, y_train = # shuffle the training dataset\n",
        "    \n",
        "    # loop through the dataset in mini-batches\n",
        "    # because we are shuffling the dataset at every point and we do not want\n",
        "    # updates on a tiny batch size we can neglect the last mini-batch that is \n",
        "    # smaller than our mini-batch nice. \n",
        "    for mb in range(...):\n",
        "        inp = x_train[..:..] # subselect the current mini-batch train examples\n",
        "        target = y_train[..:..] # subselect the current mini-batch of labels\n",
        "    \n",
        "        # as before calculate the model, gradients and do the update.\n",
        "    \n",
        "    if epoch % 1000 == 0 and epoch > 0:\n",
        "        print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLcSBOzx5M0p"
      },
      "source": [
        "Let's again visualize the losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwMFqWA05Q0j"
      },
      "source": [
        "# visalize the losses again"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRQWfpeZ2VK1"
      },
      "source": [
        "In contrast to gradient descent, we observe that the loss no longer behaves deterministically. It is a lot more \"jumpy\". In fact, if we run the same code multiple times we will also achieve a different result each time. Similarly, the final accuracy we achieve now varies. If we set the parameters right, we can observe the loss initially declining much faster and the final accuracy can be slightly higher. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqjkHMa0O20p"
      },
      "source": [
        "train_acc = ...\n",
        "test_acc = ...\n",
        "\n",
        "print(\"Training accuracy: \", train_acc)    \n",
        "print(\"Testing accuracy: \", test_acc)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUpDA43XGRl9"
      },
      "source": [
        "## What now? \n",
        "\n",
        "What can we do now? In the next week we will learn about more expressive models like random forests and neural networks that achieve better accuracies and can fit more complex tasks with more than one label. \n",
        "For now here's a couple of suggestions what we can do immediately:\n",
        "\n",
        "\n",
        "    1. Run gradient descent multiple times in a row, do the same for stochastic gradient descent, what do you observe and why? \n",
        "    2. How does the model behave if we select a different smaller/large feature set? \n",
        "    3. We have learned about more advanced gradient descent techniques such as momentum or adaptive techniques such as Adam. Change the update function and observe what happens.\n",
        "    4. We have observed earlier that the sex and class feature seem to be highly correlated with survival rate. Can you build a simple classifier by hand, based on the assumption that high passenger class female passengers survive. How does it compare to the logistic regression you have trained?"
      ]
    }
  ]
}